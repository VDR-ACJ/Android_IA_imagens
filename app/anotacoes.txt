site para treinar modelos
https://teachablemachine.withgoogle.com/train/image

Analisando as imagens da pasta
Na pasta selecionada, temos algumas imagens, como uma caneca da Alura. Ao analisar essa imagem, o aplicativo traz alguns rótulos em inglês, como "tableware, cup" (utensílio de mesa, xícara), sugerindo que é uma xícara de porcelana. No entanto, ao acessar a opção de IA, vemos que identifica um grupo de comidas, mas não há nada relacionado à nossa xícara.

Isso ocorre porque o modelo atualmente disponível não tem informações para reconhecer especificamente essa xícara. Essa situação é comum ao trabalhar com bibliotecas de reconhecimento de imagem, onde muitas vezes é necessário treinar o modelo para reconhecer objetos específicos ou criar modelos personalizados para atender a necessidades específicas.

Treinamento de modelos
Na documentação do ML Kit, há uma seção de informações sobre modelos personalizados. São mencionadas algumas opções para baixar modelos pré-treinados, como para identificar carros ou certos tipos de animais. A documentação oferece várias opções que você pode explorar, incluindo modelos disponíveis para download e instalação, bem como a possibilidade de criar seus próprios modelos personalizados.

Aprendendo a utilizar o Teachable Machine
Além disso, existe a opção de treinar seus próprios modelos para uso com o ML Kit. Para isso, podemos acessar o site https://teachablemachine.withgoogle.com/, que é parte da rede da Google. Ele oferece uma maneira mais simples para quem não quer se aprofundar tanto no treinamento de modelos, permitindo treinar rapidamente e testar os modelos em seus aplicativos.

É importante ressaltar que o campo de treinamento de modelos é bastante complexo e há cursos e formações disponíveis na Alura que tratam desses aspectos de forma mais aprofundada, incluindo como preparar os dados antes de alimentar os modelos. A abordagem da Google com o Teachable Machine é fornecer uma introdução mais simples e direta para que os usuários possam começar a entender o processo, e aprofundar conforme necessário dependendo da situação.

Ao clicarmos no botão "Get Started", destacado em azul, seremos direcionados para uma tela com três opções para treinar imagens, áudios e poses. Optaremos pela seleção de "Image Project" e escolheremos o "Standard Image Model". Embora haja um breve tutorial sobre o uso do site, clicaremos no botão "X" para fechar essa apresentação. O próximo passo é definir categorias, que funcionarão como rótulos, e associar imagens de referência para estes rótulos.

Por exemplo, vamos renomear o primeiro rótulo, atualmente denominado "Class 1", para "Android". Depois, clicaremos no botão "Upload". Vale ressaltar que há um conteúdo adicional disponível na Alura sobre Teachable Machine, explicando seu uso, incluindo a funcionalidade da webcam. No entanto, nosso foco será no processo de upload de imagens.

Após clicarmos em "Upload", selecionaremos "Choose Images from your Files" para termos acesso às pastas armazenadas em nosso computador. Já organizamos as imagens necessárias na pasta "exemplo 2", dentro de "imagens de exemplo", conforme indicado nos materiais extras. Dentro dessa pasta, encontramos uma coleção de imagens do boneco do Android na pasta "android". Selecionaremos todas as imagens desta pasta e clicaremos em "Abrir". O sistema realizará o carregamento das imagens, permitindo que continuemos com a definição de outra categoria.

Para a segunda classe, renomearemos "Class 2" como "Maça", por exemplo. Repetiremos o processo de upload, selecionando as imagens correspondentes à pasta "maça", também dentro de "exemplo 2". Selecionaremos todas as imagens e clicaremos em "Abrir". Depois, basta aguardar enquanto as imagens são carregadas. É possível que surja um breve tutorial sobre o treinamento do modelo, o qual é bastante intuitivo.

Após o envio de várias imagens para as categorias "Android" e "Maça", podemos iniciar o processo de treinamento clicando em "Train Model" e aguardar sua conclusão. Quanto maior o número de imagens, melhor será o resultado do modelo e mais tempo será necessário para o treinamento. Neste caso, o processo foi relativamente rápido. Ao finalizar, o sistema oferece a opção de exportar o modelo, mas não faremos isso agora.

Testando o modelo
Uma funcionalidade interessante é a possibilidade de trocar o método de entrada de "Webcam" para "File". Assim como no exemplo anterior, podemos arrastar uma imagem para o campo designado e o sistema tentará identificar se ela pertence à categoria "Android" ou "Maça".

Então, vamos alterar para "File" e selecionar uma imagem de teste da pasta "Imagens de Exemplo". Nesta pasta, além de outras subpastas, temos algumas imagens, dentre elas a imagem "teste-android.jpg". Vamos selecioná-la e clicar em "Abrir".

Embora esta imagem especificamente não tenha sido previamente incluída no modelo, com base no treinamento anterior, o sistema atribuirá uma alta probabilidade de 100% à categoria "Android". Da mesma forma, ao testarmos a imagem "teste-caneca-e-maca.jpg", de uma maçã com uma xícara, a certeza de pertencer à categoria "Maça" é de 98%.

Adicionando mais classes
Vamos adicionar mais duas categorias, clicando em "Add a class". A primeira será "Caneca Comum" e nela selecionaremos todas as imagens da pasta "caneca comum", dentro de "exemplo 2".

Depois, criaremos a classe "Caneca Alura" e selecionaremos as imagens da pasta "caneca preta". Após o envio das imagens, iniciaremos novamente o treinamento do modelo e aguardaremos sua conclusão.

Testando novamente o modelo
Após o treinamento do nosso modelo, podemos realizar mais um teste. Vamos alterar de "Webcam" para "File" e escolher a imagem "teste-caneca-alura-1.jpg". Ao analisar a imagem, o modelo atribui uma probabilidade de 84% para a categoria "Caneca Comum" em vez de "Caneca Alura". Isso ocorre devido à semelhança das imagens utilizadas no treinamento, as quais se assemelham mais à "Caneca Comum". No entanto, é importante notar que a cor da caneca Alura pode variar, tornando-a mais parecida com outros exemplos utilizados no treinamento.

Apesar disso, nosso modelo está bem treinado e atende aos nossos requisitos. Existem opções avançadas disponíveis, como a possibilidade de retreinamento do modelo, adição ou remoção de imagens para aprimorar ainda mais seu desempenho. Quanto mais imagens semelhantes forem adicionadas, melhor será o resultado dos testes realizados.

Exportando o modelo
Ao clicarmos em "Export Model" e selecionarmos "TensorFlow Lite" com a opção Model Conversion Type configurada para "Quantized", o modelo será convertido e estará disponível para download. Este arquivo poderá ser utilizado posteriormente com o ML Kit, conforme exploraremos a seguir.

Após o término da conversão, o modelo será baixado automaticamente. Vamos aprender como incorporar este arquivo em nosso aplicativo nas próximas etapas. Para mais informações sobre como utilizar este modelo, existem instruções detalhadas nesta página, embora nosso método de utilização possa diferir um pouco. Na sequência, entenderemos essas diferenças e aprenderemos como utilizar.




Para criar os modelos customizados você precisa:

Abrir o site Teachable Machine:
Escolha a opção de imagens;
Crie as 4 classes para cada categoria;
Adicione imagens para cada categoria;
Treine o modelo;
Faça o download no formato específico.


O Teachable Machine é um projeto da Google que visa facilitar ao máximo a criação e implantação de modelos de Machine Learning. Em aula, mencionamos algumas de suas funções, mas há outras que não foram abordadas. Ainda assim, é uma ferramenta mais simples e intuitiva de usar.

Porém, existe todo uma área dentro do estudo de Machine Learning que envolve classificação e treinamento de modelos com ferramentas e códigos mais específicos. Isso inclui questões de refinamento para tornar os modelos mais precisos e eficazes.

Aqui na Alura, temos uma formação de entrada sobre esses tópicos chamada Formação Machine Learning com Python: Classificação, e também uma formação mais avançada chamada Formação Machine Learning. E se você quiser aprender mais sobre o Teachable Machine, pode conferir também o Alura Mais Visão Computacional.

========================================================

Instalação da biblioteca
Essa documentação é semelhante à que vimos para a biblioteca com o modelo comum, mas contém códigos específicos para modelos customizados. Vamos procurar a seção "Antes de Começar", que descreve como instalar essa biblioteca, cujo processo é semelhante ao que já fizemos anteriormente. Vamos copiar a linha implementation 'com.google.mlkit:image-labeling-custom:17.0.2'.

Agora, abriremos nosso projeto no Android Studio e localizaremos o arquivo build.gradle.kts do módulo app. Na linha 87, abaixo da implementação existente, colaremos a linha copiada, substituindo as aspas simples por aspas duplas e envolvendo o trecho entre parênteses.

implementation ("com.google.mlkit:image-labeling-custom:17.0.2")
Copiar código
Em seguida, pressionaremos "Alt + Enter" e selecionaremos a opção "Replace with new library catalog...". Feito isso, sincronizamos o projeto clicando em "Sync Now", na barra superior que surgiu, e aguardamos a conclusão do processo de sincronização.

Extraindo e inserindo os arquivos do modelo
Nosso projeto já fez o download da dependência, agora precisamos inserir esse modelo no nosso projeto para utilizá-lo no código. Vamos começar extraindo os arquivos do arquivo zip que baixamos do Teachable Machine. Ao clicar com o botão direito do mouse no arquivo no gerenciador de arquivos, selecionaremos a opção "Extrair Tudo". Isso nos fornecerá os arquivos labels.txt e model.tflite.

Vamos copiar esses dois arquivos e colá-los no diretório raiz do nosso projeto no Android Studio. Para isso, selecionamos o diretório no menu lateral do Android Studio, clicamos com o botão direito do mouse e vamos em "New" > "Folder" > "Assets Folder" para criar uma nova pasta chamada "assets". Em seguida, essa pasta aparecerá no menu lateral, então basta selecioná-la e usar o atalho "Ctrl + V" para colar os arquivos. Ao fazer isso, um dos arquivos é automaticamente aberto.

Note que no arquivo labels.txt temos os rótulos que criamos no Teachable Machine:

0 Android
1 Maça
2 Caneca Comum
3 Caneca Alura
Copiar código
Ao clicar no arquivo model.tflite, que se trata do modelo em si, não temos nenhuma informação relevante.

Configurando o ImageClassifier:
Com os arquivos dentro do projeto, podemos fazer algumas alterações no código de ImageClassifier.kt. Primeiramente, vamos substituir o uso do labeler padrão pelo nosso novo modelo. No método classifyImage, na linha 26, criaremos uma variável customModel, que receberá uma instância de LocalModel.Builder(). Neste momento, é importante teclar "Alt + Enter" sobre LocalModel e importá-lo.

val customModel = LocalModel.Builder()
Copiar código
Na linha seguinte, passaremos .setAssetFilePath(), com o caminho do nosso modelo. Uma vez que armazenamos o modelo em uma pasta específica para recursos do projeto, a assets, podemos simplesmente passar seu nome dentro da string: "model.tflite". Como estamos lidando com um Builder, podemos finalizar com .build().

val customModel = LocalModel.Builder()
    .setAssetFilePath("model.tflite")
    .build()
Copiar código
Agora, precisamos passar esse modelo com algumas opções para realizar a análise. Então, vamos criar uma variável chamada customOptions, que será do tipo CustomImageLabelerOptions. Ela também é um builder, então vamos adicionar .Builder() e passar customModel.

val customOptions = CustomImageLabelerOptions.Builder(customModel)
Copiar código
Na linha seguinte, vamos adicionar o parâmetro de confiança .setConfidenceThreshold(). Por padrão, ele sugere 0.7, mas, por enquanto, vamos definir como 0.2f. Lembre-se que o f é usado para indicar que é um valor float.

Temos a opção de adicionar mais um parâmetro que é a quantidade máxima de rótulos que o modelo vai retornar, mas para esse primeiro teste não faremos isso. Vamos apenas adicionar .build() para finalizar a construção das opções.

val customOptions = CustomImageLabelerOptions.Builder(customModel)
    .setConfidenceThreshold(0.2f)
    .build()
Copiar código
Agora que as customOptions estão prontas, podemos substituir as opções padrão, que estão no método .getClient(), por customOptions.

val labeler = ImageLabeling.getClient(customOptions)
Copiar código
Executando e analisando o resultado
Se rodarmos o nosso aplicativo, vamos notar pelo menos uma diferença, então vamos pressionar "Shift + F10".

Nosso aplicativo rodou e aqui há um detalhe: como adicionamos uma nova biblioteca, o projeto agora tem alguns arquivos adicionais para ler. Esse primeiro build, após as adições, pode demorar um pouco mais, mas depois ele voltará à mesma velocidade de antes.

Agora vamos verificar se ele conseguiu identificar as imagens com base no nosso modelo personalizado. Ao clicar na maçã, não está mostrando nada. Ao clicar na foto do Android, que testamos e vimos que funcionou no Teachable Machine, também não veremos nada.

No entanto, ao voltarmos ao Android Studio e observarmos o Logcat, podemos notar que, pelo menos lá, essas informações estão aparecendo:

0.99689375

0.99609375

0.97265625

0.859375

Embora não mostre corretamente o rótulo, pois está exibindo um espaço em branco, ele é capaz de fornecer algumas porcentagens, como 99%, 97% e 85%. Isso indica que o modelo está funcionando, mas ainda não consegue nos fornecer o nome de cada rótulo. Existem algumas maneiras de resolvermos esse problema, e é isso que vamos explorar a seguir.

=====================================

Entendendo a segunda abordagem
Quando abrimos o arquivo model.tflite, aparece um aviso informando que não há metadados disponíveis no modelo e podemos clicar em "Add metadata to your model". Ao clicar, será aberta uma documentação fornecida pelo próprio TensorFlow, que é uma tecnologia desenvolvida pela equipe do Google e amplamente utilizada em vários aplicativos da empresa.

Esta documentação fornece uma explicação mais detalhada sobre por que os arquivos estão separados, como podemos juntá-los e outros detalhes relevantes. Ela inclui até mesmo uma imagem ilustrativa mostrando a separação do modelo do TensorFlow e dos metadados associados a ele. No nosso caso, este metadados são os rótulos contidos no arquivo labels.txt.

Além disso, a documentação menciona um site do Google onde podemos combinar esses dois arquivos. Podemos acessá-lo através da página de metadados do TensorFlow Lite.

Ao clicar no link, uma nova documentação é aberta. Note que temos a opção tanto de executar esse código localmente por questões de privacidade, se preferirmos, quanto de usar o "Run in Google Colab" fornecido pelo Google. Esse é essencialmente um ambiente virtual pré-configurado com tudo o que precisamos para realizar a junção dos arquivos. Podemos acessá-lo remotamente, enviar nossos arquivos, executar as etapas necessárias, baixar o resultado e, em seguida, tudo será excluído.

A documentação também detalha quais recursos estarão disponíveis nesse ambiente virtual, mas podemos simplesmente clicar em "Run in Google Colab" para iniciar o processo.

Implementando a abordagem no Google Colab
Ao clicarmos nessa opção, o Google Colab será carregado. A primeira opção, localizada na seção "Create Model Metadata for Task Library and Codegen", é a que precisamos. Antes dela, temos a seção "Prerequisites", com a célula de código pip install tf.lite-support.nightly. Ao lado desse texto, há um pequeno botão semelhante a um botão de reprodução. Clicamos nele e aguardamos a instalação dessa biblioteca, que será necessária para começarmos a combinar nossos dois arquivos.

Esse processo pode levar algum tempo, dependendo da velocidade da sua conexão com a internet. Se tudo funcionar corretamente, aparecerá um símbolo verde de verificação à esquerda da célula. Em caso de erro, você pode clicar novamente no botão de reprodução e ele deverá executar o comando novamente.

Agora, na seção seguinte, precisaremos fazer a importação de algumas coisas dessa biblioteca. Vamos executar a célula de "Step 1", onde temos a importação de alguns pacotes requeridos.

from tflite_support.metadata_writers import image_classifier
from tflite_support.metadata_writers import writer_utils
Copiar código
Caso você queira entender mais sobre o Google Colab, há material disponível na plataforma da Alura.

Após a execução, temos o "Step 2", mas não precisamos executá-lo porque ele faria o download de um modelo de teste do próprio Google para mostrar como isso funciona. Então passemos para o "Step 3".

Na célula do terceiro passo, há alguns códigos onde precisamos alterar o caminho do modelo que queremos utilizar. Dentro desse código, vamos precisar arrastar nossos dois arquivos, labels.txt e model.tflite, que contém os rótulos e o modelo em si, respectivamente.

Vamos abrir a pasta onde consta esses arquivos, selecioná-los e arrastálos para a seção de arquivos do Google Colab. Ao fazer isso, aparecerá um aviso informando que, após serem utilizados, esses arquivos serão excluídos. Vamos confirmar clicando em "OK".

Agora, precisamos alterar o código do "Step 3". Em _MODEL_PATH, vamos passar "model.tflite", que é o nome do nosso arquivo de modelo. Em _LABEL_FILE, vamos passar "labels.txt", que é o nome do nosso arquivo de rótulos. Em _SAVE_TO_PATH, podemos passar um nome de nossa preferência. Nesse caso, optaremos por "custom-model.tflite".

ImageClassifierWriter = image_classifier.Metadatawriter
_MODEL_PATH = "model.tflite"
_LABEL_FILE = "labels.txt"
_SAVE_TO_PATH = "custom_model.tflite"
Copiar código
Em seguida, podemos clicar no botão de play para rodar essa célula e aguardar o resultado.

Se não aparecer automaticamente na seção de arquivos na lateral esquerda, basta clicar na pasta com o símbolo de atualizar e deve aparecer nosso modelo completo.

Depois, clicamos nos três pontinhos ao lado de custom_model.tflite e selecionamos a opção "Fazer download". Vamos optar por salválo no mesmo local de antes e o modelo será baixado já com os metadados.

Importando o novo modelo
Entendido. Agora, vamos importar esse novo modelo para o Android Studio. Para isso, copiaremos custom_model.tflite de nossa máquina e o colaremos na pasta assets dentro do Android Studio.

Ao abrir o arquivo, observe que a mensagem é um pouco diferente. Ele já exibe os metadados que foram inseridos neste modelo.

Fazendo as alterações necessárias
De volta ao arquivo ImageClassifier.kt, em setAssetFilePath(), na linha 29, vamos alterar "model.tflite" para "custom_model.tflite".

val customModel = LocalModel.Builder()
    .setAssetFilePath("custom_model.tflite")
    .build()
Copiar código
Há algumas linhas de código que não precisaremos mais, mas você pode mantê-las caso deseje estudar. São elas as linhas 41 a 41, onde temos a variável newLabels. Não precisamos mais ler o arquivo labels.txt, então ele não precisa continuar na pasta. Mas, se preferir, pode mantê-lo para fins de estudo.

Note que surgirá um erro nas linhas 42 e 45, onde há newLabels, então podemos remover as ocorrências dessa variável. Em seguida, substituiremos it.index por it.text, tanto na linha 42, quanto na linha 45. Por enquanto, manteremos o .substring(2).

labels.forEach {
                    val labelAndConfidence = "${it.text.substring(2)} - ${it.confidence}"
                    Log.d("ImageDetailScreen", labelAndConfidence)
                }
                onSuccess(labels.map { it.text.substring(2) })
            }
            .addOnFailureListener { onFail() }
    }
Copiar código
Executando a aplicação
Nosso código não apresenta mais nenhum erro, então podemos executar o aplicativo com "Shift + F10" e aguardar para ver o resultado.

Nosso aplicativo rodou e, infelizmente, não teremos nenhuma mudança visual. Podemos perceber que, ao clicar em cada um dos arquivos, a forma como os rótulos estão sendo exibidos permanece a mesma. Mas, agora, dependendo da situação, podemos adicionar um arquivo com centenas, dezenas, ou milhares de categorias.

Passamos pelo caminho difícil de ter que ler um arquivo, mas com este código e método, conseguimos algo um pouco mais performático. A seguir, faremos o ajuste para que a parte de IA funcione corretamente com esta nossa nova implementação do modelo customizado.

https://www.tensorflow.org/lite/models/convert/metadata_writer_tutorial
https://www.tensorflow.org/lite/models/convert/metadata?hl=pt-br#pack_the_associated_files

Para que possamos juntar os dois arquivos referente ao modelo do Tensorflow Lite que temos hoje separados:

Abra a página TensorFlow Lite Metadata Writer API e clique na opção “Run in Google Colab” na página que irá se abrir:

Clique no botão de play ao lado do texto: !pip install tflite-support-nightly para que essa celular de código seja executada e aguarde.
Localize a seção “Image classifiers” e execute o passo “Step 1: Import the required packages” aguarde a execução.
Na célula do passo “Step 3” altere os valores das variáveis “_MODEL_PATH” e “_LABEL_FILE” para os nomes dos arquivos que vamos fazer o upload e “_SAVE_TO_PATH” para qual nome o novo arquivo do modelo deve ter.
Fazendo upload do nosso modelo

No canto esquerdo da página, localize o ícone uma pasta que deve mostrar o texto “arquivos” ao passar o mouse por cima, clique nela.
Arraste do seu computador para essa aba de arquivos, o nosso modelo e seu arquivo de rótulos.
Após o upload de nossos arquivos, execute a célula do passo “Step 3: Create metadata writer and populate.”, aguarde o resultado.
Se tudo deu certo, você verá que além dos arquivos que enviamos, na aba de arquivos um novo deve ter aparecido, se não apareceu clique no icone de “atualizar” abaixo do nome “Arquivos”.
Faça o download do arquivo gerado clicando clicando nos 3 pontinhos do canto direito dele.
Volte ao Android e:

Coloque o modelo atualizado que acabamos de baixar na pasta assets.
Dentro da classe ImageClassifier, troque o nome do nosso modelo antigo por esse novo.
Remova os códigos que faziam a leitura manual do arquivo “labels.txt”.
As referências de newLabels não sejam mais necessárias, então ajuste os pontos em que ela era usada para que utilizem apenas labels comuns, como antes.

========